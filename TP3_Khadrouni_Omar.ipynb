{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkkQ/nZNX95x1BjvbEcKFy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OMAR200002/NLP/blob/main/TP3_Khadrouni_Omar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vctq9xCLbXPR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d21e2f2-88c7-448a-f774-6f960cfedff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Charger le Dataset"
      ],
      "metadata": {
        "id": "BJxgiKI3tH3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('movie_review.csv')\n",
        "\n",
        "texts = df['text']\n",
        "tags = df['tag']"
      ],
      "metadata": {
        "id": "WTuV2SbMcwgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-traitement"
      ],
      "metadata": {
        "id": "TlBQa5FVt0xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    filtered_words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
        "    return filtered_words\n",
        "\n",
        "preprocessed_texts = [preprocess_text(text) for text in texts]\n",
        "print(preprocessed_texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXMrtwEhiCY0",
        "outputId": "062ffa4b-18bb-4a50-fad5-40ff3325ea83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', \"'re\", 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', \"'s\", 'never', 'really', 'comic', 'book', 'like', 'hell']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraînement du Modèle Word2Vec\n"
      ],
      "metadata": {
        "id": "UDUuq7Gbdgl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = Word2Vec(preprocessed_texts, vector_size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "UoR8vZYGdoQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorisation des Textes"
      ],
      "metadata": {
        "id": "bPcT97jaup2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, model, size=100):\n",
        "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
        "\n",
        "    if len(vectors) > 0:\n",
        "        vector = np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        # Si aucun mot n'est reconnu, renvoyer un vecteur de zéros\n",
        "        vector = np.zeros(size)\n",
        "\n",
        "    return vector\n",
        "\n",
        "vectorized_texts = [vectorize_text(text, word2vec_model) for text in preprocessed_texts]\n",
        "\n",
        "# Convertir en array numpy pour éviter les problèmes de dimensionnalité\n",
        "vectorized_texts = np.array(vectorized_texts)\n",
        "print(vectorized_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h3AbQ8Pi3yC",
        "outputId": "45b635fe-8213-4b9a-d058-e7285baa7344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.4068065   0.71108294  0.41838363 ... -0.55216956 -0.02672342\n",
            "   0.1244285 ]\n",
            " [-0.16390195  0.53877532  0.33321279 ... -0.26485941 -0.0094007\n",
            "   0.29539573]\n",
            " [-0.28882986  0.7675522   0.40230328 ... -0.60826433 -0.10781863\n",
            "   0.20013359]\n",
            " ...\n",
            " [-0.69750112  0.97282124  0.76180422 ... -0.6776737  -0.42151785\n",
            "   0.3792586 ]\n",
            " [-0.16064388  0.48637897  0.4668203  ... -0.42024243 -0.04850768\n",
            "   0.22736402]\n",
            " [-0.59965068  0.84847343  0.43310422 ... -0.6950165  -0.34589958\n",
            "  -0.03605396]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Division des Données"
      ],
      "metadata": {
        "id": "WSnuafFpvGK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(vectorized_texts, tags, test_size=0.2)"
      ],
      "metadata": {
        "id": "CSbDSoQDjnPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construction et Évaluation du modéle"
      ],
      "metadata": {
        "id": "DzhxM9XGvthJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "id": "-y0S3YZ6jbeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef7c0ae2-e46f-4484-bc46-f3cb2508e1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.56      0.47      0.51      6373\n",
            "         pos       0.56      0.65      0.60      6571\n",
            "\n",
            "    accuracy                           0.56     12944\n",
            "   macro avg       0.56      0.56      0.56     12944\n",
            "weighted avg       0.56      0.56      0.56     12944\n",
            "\n"
          ]
        }
      ]
    }
  ]
}